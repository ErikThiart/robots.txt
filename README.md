# robots.txt
Here is a long list of utterly useless web robots eating your bandwidth for no good reason.

There is currently a total of ~582 robots disallowed; at least, they follow standards, but, unfortunately, many don't follow any standard & cause unnecessary load on servers.

Various SEO bot services are the primary culprits. They are very aggressive with crawling, scanning pages and offer nothing in return to you. Instead, they often times aggregate all your data and sell it on their websites as a competitor's analysis. 

It is a waste of bandwidth.

If you are a website operator, feel free to use my robots.txt file & stop those bots.

Feel free to add bots to this file as you come accross them in your logs.
